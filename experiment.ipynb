{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "import praw\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"), \n",
    ")\n",
    "\n",
    "def get_embedding(txts):\n",
    "    response = client.embeddings.create(\n",
    "        input=txts,\n",
    "        model=\"text-embedding-3-small\"\n",
    "        )\n",
    "    return response.data\n",
    "\n",
    "# response = client.embeddings.create(\n",
    "#     input=\"Your text string goes here\",\n",
    "#     model=\"text-embedding-3-small\"\n",
    "# # )\n",
    "\n",
    "# print(response.data[0].embedding)\n",
    "embeddings_dir = \"embeddings/\"\n",
    "\n",
    "reddit_subs_limit = 2000\n",
    "reddit_top_all_post_limit = 150\n",
    "reddit_hot_post_limit = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings_for_posts(sub, posts, chunk_size=25):\n",
    "    sub_filename = f\"{embeddings_dir}{sub}.pickle\"\n",
    "    if not os.path.exists(sub_filename):\n",
    "        existing_df = pd.DataFrame(columns=[\"Id\", \"Sub\", \"Title\", \"Embedding\", \"Added\"])\n",
    "    else:\n",
    "        existing_df = pd.read_pickle(sub_filename)\n",
    "\n",
    "    existing = set(existing_df[\"Id\"])\n",
    "    new_posts = [post for post in posts if post.id not in existing]\n",
    "    for i in range(0, len(new_posts), chunk_size):\n",
    "        chunk = new_posts[i:i+chunk_size]\n",
    "        results = get_embedding([post.title for post in chunk])\n",
    "        for post, res in zip(chunk, results):\n",
    "            existing_df.loc[len(existing_df)] = [\n",
    "                post.id,\n",
    "                sub,\n",
    "                post.title,\n",
    "                res.embedding,\n",
    "                datetime.datetime.now().isoformat()\n",
    "            ]\n",
    "\n",
    "    existing_df.to_pickle(sub_filename)\n",
    "    return {\n",
    "        \"fetched\": len(new_posts),\n",
    "        \"skipped\": len(posts) - len(new_posts),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def get_combined_embeddings(only_subs):\n",
    "    file_list = glob.glob(f\"{embeddings_dir}*.pickle\")\n",
    "    all_dfs = []\n",
    "    for file_path in file_list:\n",
    "        if len(only_subs) > 0:\n",
    "            should_read = False\n",
    "            for sub in only_subs:\n",
    "                if file_path.endswith(f\"{sub}.pickle\"):\n",
    "                    should_read = True\n",
    "                    break\n",
    "            if not should_read:\n",
    "                continue\n",
    "        df_temp = pd.read_pickle(file_path)\n",
    "        all_dfs.append(df_temp)\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    # combined_df[\"Embedding_np\"] = combined_df[\"Embedding\"].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "    return combined_df\n",
    "\n",
    "def for_each_sub_embeddings():\n",
    "    file_list = glob.glob(f\"{embeddings_dir}*.pickle\")\n",
    "    for file_path in file_list:\n",
    "        df_temp = pd.read_pickle(file_path)\n",
    "        yield df_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Sunday Daily Thread: What's everyone working on this week?\", '1hjmlmy'),\n",
       " ('Thursday Daily Thread: Python Careers, Courses, and Furthering Education!',\n",
       "  '1hmc71n'),\n",
       " (\"Lad wrote a Python script to download Alexa voice recordings, he didn't expect this email.\",\n",
       "  'g53lxf'),\n",
       " ('This post has:', 'hoolsm')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=os.environ.get(\"REDDIT_CLIENT_ID\"),\n",
    "    client_secret=os.environ.get(\"REDDIT_CLIENT_SECRET\"),\n",
    "    user_agent='Python:com.findthesub:fetch-script'\n",
    ")\n",
    "\n",
    "def fetch_hot_and_top_posts(sub, config = {\n",
    "    'hot_count': reddit_hot_post_limit,\n",
    "    'top_all_count': reddit_top_all_post_limit\n",
    "}):\n",
    "    max_chunk = 200\n",
    "    hot_count = config['hot_count']\n",
    "    top_all_count = config['top_all_count']\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    collected = []\n",
    "    for sort, limit, kwargs in [\n",
    "        ('hot', hot_count, {}),\n",
    "        ('top', top_all_count, {'time_filter':'all'})\n",
    "    ]:\n",
    "        after = None\n",
    "        fetched = 0\n",
    "        while fetched < limit:\n",
    "            chunk_size = min(max_chunk, limit - fetched)\n",
    "            gen = getattr(subreddit, sort)(limit=chunk_size, params={'after': after}, **kwargs)\n",
    "            items = list(gen)\n",
    "            if not items: break\n",
    "            collected.extend(items)\n",
    "            fetched += len(items)\n",
    "            after = items[-1].name\n",
    "            if len(items) < chunk_size: break\n",
    "    return collected\n",
    "\n",
    "def get_top_subs(limit=reddit_subs_limit):\n",
    "    max_chunk = 100\n",
    "    collected = []\n",
    "    after = None\n",
    "    \n",
    "    fetched = 0\n",
    "    while fetched < limit:\n",
    "        chunk_size = min(max_chunk, limit - fetched)\n",
    "        gen = reddit.subreddits.popular(limit=chunk_size, params={'after': after})\n",
    "        items = list(gen)\n",
    "        if not items:\n",
    "            break\n",
    "        collected.extend(items)\n",
    "        fetched += len(items)\n",
    "        # For subreddits, the 'fullname' should be used as 'after'\n",
    "        # but if that doesn't work, try items[-1].name or items[-1].id\n",
    "        after = items[-1].fullname  \n",
    "        if len(items) < chunk_size:\n",
    "            break\n",
    "\n",
    "    return [s.display_name for s in collected]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# titles = fetch_reddit_post_titles_for_sub('python', limit=120, sort='new')\n",
    "# print(titles)\n",
    "result = fetch_hot_and_top_posts('python', config = {\n",
    "    'hot_count': 2,\n",
    "    'top_all_count': 2\n",
    "})\n",
    "[(x.title, x.id) for x in result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a261e4da62349ae932e68350ab49e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='0/0 processed')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be322d9a39554b05b5d0f5c5c8e97681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9452b0a0bcac4dc5bd7578400dd06a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Logs', disabled=True, layout=Layout(height='300px', width='100%'), placeholderâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "progress = widgets.IntProgress(value=0, )\n",
    "progress_label = widgets.Label(value=f\"0/0 processed\")\n",
    "log_area = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"Logs will appear here...\",\n",
    "    description=\"Logs\",\n",
    "    layout=widgets.Layout(width='100%', height='300px'),\n",
    "    disabled=True,\n",
    ")\n",
    "display(progress_label, progress, log_area)\n",
    "\n",
    "log_area.value += \"Fetching top subreddits...\\n\"\n",
    "subs_to_fetch = get_top_subs()[714:]\n",
    "log_area.value += f\"Fetched {len(subs_to_fetch)} subreddits\\n\\n\"\n",
    "progress.max = len(subs_to_fetch)\n",
    "progress.value = 0\n",
    "progress_label.value = f\"0/{len(subs_to_fetch)} processed\"\n",
    "\n",
    "\n",
    "for sub in subs_to_fetch:\n",
    "    progress.value += 1 \n",
    "    progress_label.value = f\"{progress.value}/{len(subs_to_fetch)} processed\"\n",
    "    \n",
    "    # Update logs\n",
    "    log_message = f\"Processing '{sub}'...\\n\"\n",
    "    posts = fetch_hot_and_top_posts(sub)\n",
    "    log_message += f\"Fetched {len(posts)} titles for {sub}\\n\"\n",
    "    \n",
    "    results = add_embeddings_for_posts(sub, posts)\n",
    "    log_message += f\"Added {results['fetched']} new embeddings, skipped {results['skipped']} existing\\n\\n\"\n",
    "    \n",
    "    log_area.value += log_message  # Append to the textarea\n",
    "    \n",
    "    log_area_lines = log_area.value.split(\"\\n\")\n",
    "    if len(log_area_lines) > 500:\n",
    "        log_area.value = \"\\n\".join(log_area_lines[-500:])\n",
    "\n",
    "# Final step\n",
    "# df = get_combined_embeddings(subs_to_fetch)\n",
    "# df.to_pickle(\"embeddings.pickle\")\n",
    "log_area.value += \"Processing complete. Saved embeddings to 'embeddings/' directory\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial 200\n",
      "CasualPH 200\n",
      "askgaybros 200\n",
      "nyc 200\n",
      "granturismo 200\n",
      "RocketLeague 200\n",
      "LogitechG 200\n",
      "distressingmemes 200\n",
      "anime 200\n",
      "minnesota 200\n",
      "keyboards 200\n",
      "TIHI 200\n",
      "Tekken 200\n",
      "LooksmaxingAdvice 200\n",
      "StreetFighter 200\n",
      "cycling 200\n",
      "holdmycatnip 200\n",
      "hiphopheads 200\n",
      "wow 200\n",
      "nutrition 200\n",
      "Sneakers 200\n",
      "starterpacks 200\n",
      "90DayFiance 200\n",
      "EASportsFC 200\n",
      "mtg 200\n",
      "OnePieceSpoilers 200\n",
      "skeptic 200\n",
      "pyrocynical 200\n",
      "EDM 200\n",
      "LeopardsAteMyFace 200\n",
      "singapore 200\n",
      "reddevils 200\n",
      "askSingapore 200\n",
      "rva 200\n",
      "AskPhotography 200\n",
      "MonsterHunterWorld 200\n",
      "hvacadvice 200\n",
      "Adulting 200\n",
      "Pathfinder_RPG 200\n",
      "FinancialPlanning 200\n",
      "indonesia 200\n",
      "comics 200\n",
      "QuebecLibre 200\n",
      "EmulationOnAndroid 200\n",
      "Anxiety 200\n",
      "ClashRoyale 200\n",
      "Netherlands 200\n",
      "CatAdvice 200\n",
      "food 200\n",
      "truespotify 200\n",
      "wholesomeyuri 200\n",
      "unrealengine 200\n",
      "jailbreak 200\n",
      "analog 200\n",
      "GenX 200\n",
      "webdev 200\n",
      "Catholicism 200\n",
      "FifaCareers 200\n",
      "Jujutsushi 200\n",
      "ProductManagement 200\n",
      "touhou 200\n",
      "OculusQuest 200\n",
      "ik_ihe 200\n",
      "teslamotors 200\n",
      "BudgetAudiophile 200\n",
      "SquaredCircle 200\n",
      "LinkedInLunatics 200\n",
      "UKPersonalFinance 200\n",
      "todayilearned 200\n",
      "criticalrole 200\n",
      "oculus 200\n",
      "wohnen 200\n",
      "antivirus 200\n",
      "CurseofStrahd 200\n",
      "love 200\n",
      "IASIP 200\n",
      "onebag 200\n",
      "BambuLab 200\n",
      "Bitwarden 200\n",
      "JapanTravel 200\n",
      "Christianity 200\n",
      "MSI_Gaming 200\n",
      "MBA 200\n",
      "RDR2 200\n",
      "raleigh 200\n",
      "summonerswar 200\n",
      "Switch 200\n",
      "DarkSouls2 200\n",
      "Spiderman 200\n",
      "BoomersBeingFools 200\n",
      "answers 200\n",
      "CarSalesTraining 200\n",
      "Asmongold 200\n",
      "dbz 200\n",
      "DevilMayCry 200\n",
      "PS5 200\n",
      "Xenoblade_Chronicles 200\n",
      "stalker 200\n",
      "cscareerquestionsEU 200\n",
      "weddingplanning 200\n",
      "StarWars 200\n",
      "Apartmentliving 200\n",
      "Forex 200\n",
      "Turkey 200\n",
      "TheGirlSurvivalGuide 200\n",
      "LastEpoch 200\n",
      "cats 200\n",
      "marvelstudios 200\n",
      "ToiletPaperUSA 200\n",
      "DeepRockGalactic 200\n",
      "LeagueOfMemes 200\n",
      "tumblr 200\n",
      "CrazyFuckingVideos 200\n",
      "Watchexchange 200\n",
      "TerrifyingAsFuck 200\n",
      "dogs 200\n",
      "TravelHacks 200\n",
      "DDLC 200\n",
      "FREEMEDIAHECKYEAH 24\n",
      "RogueTraderCRPG 200\n",
      "Residency 200\n",
      "coaxedintoasnafu 200\n",
      "britishproblems 200\n",
      "HazbinHotel 200\n",
      "UnearthedArcana 200\n",
      "VictoriaBC 200\n",
      "VirtualYoutubers 200\n",
      "CHIBears 200\n",
      "TNOmod 200\n",
      "gardening 200\n",
      "reddeadredemption 200\n",
      "DarkAndDarker 200\n",
      "boston 200\n",
      "PixelArt 200\n",
      "college 200\n",
      "LetsTalkMusic 200\n",
      "AskHR 200\n",
      "crossfit 200\n",
      "thebachelor 200\n",
      "Minecraftbuilds 200\n",
      "headphones 200\n",
      "WGU 200\n",
      "Art 200\n",
      "prusa3d 200\n",
      "arknights 200\n",
      "MadeMeSmile 200\n",
      "f150 200\n",
      "discordapp 200\n",
      "CrappyDesign 200\n",
      "whatsthisbug 200\n",
      "popheadscirclejerk 200\n",
      "Pikabu 200\n",
      "Twitter_Brasil 200\n",
      "coys 200\n",
      "trees 200\n",
      "sadcringe 200\n",
      "CompTIA 200\n",
      "OMORI 200\n",
      "Cornell 200\n",
      "fatestaynight 200\n",
      "wownoob 200\n",
      "SpyxFamily 200\n",
      "AskNYC 200\n",
      "remotework 200\n",
      "FL_Studio 200\n",
      "indiadiscussion 200\n",
      "tjournal_refugees 200\n",
      "NintendoSwitch 200\n",
      "redditonwiki 200\n",
      "Fedora 200\n",
      "unitedkingdom 200\n",
      "Bolehland 200\n",
      "Munich 200\n",
      "SipsTea 200\n",
      "travisscott 200\n",
      "eu4 200\n",
      "Entrepreneur 200\n",
      "Berserk 200\n",
      "Gunners 200\n",
      "pcmasterrace 200\n",
      "WouldYouRather 200\n",
      "AdviceAnimals 200\n",
      "fo4 200\n",
      "CreateMod 200\n",
      "FinancialCareers 200\n",
      "civ 200\n",
      "node 200\n",
      "HaircareScience 200\n",
      "running 200\n",
      "HypixelSkyblock 200\n",
      "tf2 200\n",
      "battlemaps 200\n",
      "notinteresting 200\n",
      "florida 200\n",
      "farmingsimulator 200\n",
      "forhonor 200\n",
      "wowhardcore 200\n",
      "IdiotsInCars 200\n",
      "AskBaking 200\n",
      "HiTMAN 200\n",
      "MovieSuggestions 200\n",
      "Studium 200\n",
      "megalophobia 200\n",
      "brisbane 200\n",
      "polandball 200\n",
      "americanairlines 200\n",
      "infinitecraft 200\n",
      "PaymoneyWubby 200\n",
      "airsoft 200\n",
      "CookieClicker 200\n",
      "EatCheapAndHealthy 200\n",
      "opensource 200\n",
      "chemistry 200\n",
      "Music 200\n",
      "klippers 200\n",
      "CPTSD 200\n",
      "medicalschool 200\n",
      "CBSE 200\n",
      "questions 200\n",
      "LosAngeles 200\n",
      "grandorder 200\n",
      "elderscrollsonline 200\n",
      "ApexUncovered 200\n",
      "notliketheothergirls 200\n",
      "dividends 200\n",
      "dubai 200\n",
      "HFY 200\n",
      "offerup 200\n",
      "Ningen 200\n",
      "HENRYfinance 200\n",
      "30PlusSkinCare 200\n",
      "verizon 200\n",
      "OffMyChestPH 200\n",
      "tragedeigh 200\n",
      "4kbluray 200\n",
      "remnantgame 200\n",
      "Falcom 200\n",
      "TankPorn 200\n",
      "drivingUK 200\n",
      "XboxGamePass 200\n",
      "ultrawidemasterrace 200\n",
      "Jokes 200\n",
      "ichbin40undSchwurbler 200\n",
      "NYCapartments 200\n",
      "exmuslim 200\n",
      "DogAdvice 200\n",
      "xbox 200\n",
      "Awww 200\n",
      "insanepeoplefacebook 200\n",
      "androiddev 200\n",
      "delhi 200\n",
      "pokemongo 200\n",
      "TheWayWeWere 200\n",
      "naturalbodybuilding 200\n",
      "golf 200\n",
      "tennis 200\n",
      "bisexual 200\n",
      "FacebookMarketplace 200\n",
      "bapcsalescanada 200\n",
      "HousingUK 200\n",
      "Bogleheads 200\n",
      "Nepal 200\n",
      "orangetheory 200\n",
      "guitars 200\n",
      "BeautyGuruChatter 200\n",
      "community 200\n",
      "harrypotter 200\n",
      "egg_irl 200\n",
      "malaysia 200\n",
      "CasualConversation 200\n",
      "Home 201\n",
      "Drumkits 200\n",
      "RealEstate 200\n",
      "IllegallySmolCats 200\n",
      "houseplants 200\n",
      "UkraineWarVideoReport 200\n",
      "NFLNoobs 200\n",
      "kpop 200\n",
      "Battlefield 200\n",
      "business 200\n",
      "trackers 200\n",
      "btd6 200\n",
      "HubermanLab 200\n",
      "fightporn 200\n",
      "VeteransBenefits 200\n",
      "aliens 200\n",
      "furry_irl 200\n",
      "xqcow 200\n",
      "DeathStranding 200\n",
      "UFOs 200\n",
      "Nijisanji 187\n",
      "bonehurtingjuice 200\n",
      "apexuniversity 200\n",
      "argentina 200\n",
      "ImaginaryWarhammer 200\n",
      "geography 200\n",
      "LowSodiumCyberpunk 200\n",
      "BlueArchive 200\n",
      "AskArgentina 200\n",
      "japan 200\n",
      "work 200\n",
      "learnmachinelearning 200\n",
      "Quebec 200\n",
      "poker 200\n",
      "AskIndia 200\n",
      "DOG 200\n",
      "baseballcards 200\n",
      "stevenuniverse 200\n",
      "cremposting 200\n",
      "NBA2k 200\n",
      "raisedbynarcissists 200\n",
      "Adelaide 200\n",
      "jobs 200\n",
      "aws 200\n",
      "macbookair 200\n",
      "starwarsmemes 200\n",
      "dadjokes 200\n",
      "FearAndHunger 200\n",
      "usajobs 200\n",
      "Wrasslin 200\n",
      "depression 200\n",
      "realmadrid 200\n",
      "AskMen 200\n",
      "insanepinoyfacebook 200\n",
      "TrueOffMyChest 200\n",
      "RunningShoeGeeks 200\n",
      "BG3 200\n",
      "C_Programming 200\n",
      "Fighters 200\n",
      "PERSoNA 200\n",
      "PPC 200\n",
      "flipperzero 200\n",
      "nope 200\n",
      "HellLetLoose 200\n",
      "h1b 200\n",
      "ExpectationVsReality 200\n",
      "Beatmatch 200\n",
      "BokuNoHeroAcademia 200\n",
      "investimentos 200\n",
      "BatmanArkham 200\n",
      "bjj 200\n",
      "Wellthatsucks 200\n",
      "wisconsin 200\n",
      "OntarioGrade12s 200\n",
      "freefolk 200\n",
      "MicrosoftTeams 200\n",
      "nvidia 200\n",
      "PUBATTLEGROUNDS 200\n",
      "WorkAdvice 200\n",
      "Cricket 200\n",
      "ipad 200\n",
      "Witcher3 200\n",
      "instacart 200\n",
      "Aliexpress 200\n",
      "JRPG 200\n",
      "Monitors 200\n",
      "geopolitics 200\n",
      "Basketball 200\n",
      "GenZ 200\n",
      "MonsterHunter 200\n",
      "CrackWatch 200\n",
      "therewasanattempt 200\n",
      "discgolf 200\n",
      "homeassistant 200\n",
      "gainit 200\n",
      "JapanTravelTips 200\n",
      "videogames 200\n",
      "blankies 200\n",
      "F1Technical 200\n",
      "Frontend 200\n",
      "Flipping 200\n",
      "Marvel 200\n",
      "CharacterAI 200\n",
      "BG3Builds 200\n",
      "CanadaHousing2 200\n",
      "LAinfluencersnark 200\n",
      "SonyAlpha 200\n",
      "AirForce 200\n",
      "mumbai 200\n",
      "starcitizen 200\n",
      "inthenews 200\n",
      "MarvelSnap 200\n",
      "Amd 200\n",
      "languagelearning 200\n",
      "OUTFITS 200\n",
      "WhitePeopleTwitter 200\n",
      "apexlegends 200\n",
      "Scams 200\n",
      "beauty 200\n",
      "web_design 200\n",
      "moviescirclejerk 200\n",
      "TVTooHigh 200\n",
      "BabyBumps 200\n",
      "fo76 200\n",
      "UkraineRussiaReport 200\n",
      "linux4noobs 200\n",
      "nostalgia 200\n",
      "deadbydaylight 200\n",
      "nosleep 200\n",
      "TikTokCringe 200\n",
      "martialarts 200\n",
      "riskofrain 200\n",
      "KerbalSpaceProgram 200\n",
      "whatisit 200\n",
      "hardware 200\n",
      "SkincareAddiction 200\n",
      "FFVIIRemake 200\n",
      "retrogaming 200\n",
      "Isekai 200\n",
      "soccercirclejerk 200\n",
      "brasilivre 200\n",
      "thewalkingdead 200\n",
      "196 200\n",
      "GenshinImpact 200\n",
      "EntitledPeople 200\n",
      "StudentLoans 200\n",
      "de 200\n",
      "Futurology 200\n",
      "lakers 200\n",
      "PhD 200\n",
      "AskPhysics 200\n",
      "actuallesbians 200\n",
      "smashbros 200\n",
      "IWantToLearn 200\n",
      "solana 200\n",
      "drums 200\n",
      "yakuzagames 200\n",
      "unpopularopinion 200\n",
      "GlowUps 200\n",
      "Nicegirls 200\n",
      "vscode 200\n",
      "dccomicscirclejerk 200\n",
      "AfterEffects 200\n",
      "ElderScrolls 200\n",
      "horror 200\n",
      "AndroidQuestions 200\n",
      "SanJose 200\n",
      "sex 200\n",
      "Brawlstars 200\n",
      "Warthunder 200\n",
      "TheBear 200\n",
      "msp 200\n",
      "GamingLaptops 200\n",
      "NameNerdCirclejerk 200\n",
      "GeForceNOW 200\n",
      "TheOwlHouse 200\n",
      "magicTCG 200\n",
      "step1 200\n",
      "thelastofus 200\n",
      "networking 200\n",
      "steelseries 200\n",
      "linux 200\n",
      "UIUC 200\n",
      "GTA6 200\n",
      "VideoEditing 200\n",
      "rugbyunion 200\n",
      "CarTalkUK 200\n",
      "shortcuts 200\n",
      "UberEATS 200\n",
      "guns 200\n",
      "geometrydash 200\n",
      "videography 200\n",
      "taskmaster 200\n",
      "PewdiepieSubmissions 200\n",
      "Anki 200\n",
      "developersIndia 200\n",
      "Eminem 200\n",
      "BehindTheClosetDoor 200\n",
      "frugalmalefashion 200\n",
      "perfectlycutscreams 200\n",
      "nederlands 200\n",
      "GalaxyWatch 200\n",
      "de_EDV 200\n",
      "ReadyOrNotGame 200\n",
      "Cruise 200\n",
      "Guildwars2 200\n",
      "LongDistance 200\n",
      "SCJerk 200\n",
      "incremental_games 200\n",
      "ClassroomOfTheElite 200\n",
      "meirl 200\n",
      "MicrosoftFlightSim 200\n",
      "PoliticalCompassMemes 200\n",
      "HongKong 200\n",
      "askscience 200\n",
      "spotify 200\n",
      "BestofRedditorUpdates 200\n",
      "vinyl 200\n",
      "lostarkgame 200\n",
      "hotels 200\n",
      "ukraine 200\n",
      "selfimprovement 200\n",
      "doordash 200\n",
      "jellyfin 200\n",
      "wholesomememes 200\n",
      "yurimemes 200\n",
      "memes 200\n",
      "AITA_WIBTA_PUBLIC 200\n",
      "ObsidianMD 200\n",
      "minecraftsuggestions 200\n",
      "UCSD 200\n",
      "bodyweightfitness 200\n",
      "SparkleMains 200\n",
      "afkarena 200\n",
      "OpenAI 200\n",
      "OtomeIsekai 200\n",
      "Proxmox 200\n",
      "2westerneurope4u 200\n",
      "copypasta 200\n",
      "TaylorSwift 200\n",
      "google 200\n",
      "Boruto 200\n",
      "football 200\n",
      "Windows10 200\n",
      "dragonage 200\n",
      "findapath 200\n",
      "flying 200\n",
      "FinalFantasy 200\n",
      "fragrance 200\n",
      "Hololive 200\n",
      "privacy 200\n",
      "ApplyingToCollege 200\n",
      "wirklichgutefrage 200\n",
      "RomanceBooks 200\n",
      "ifyoulikeblank 200\n",
      "synthesizers 200\n",
      "KidsAreFuckingStupid 200\n",
      "masseffect 200\n",
      "Military 200\n",
      "okbuddychicanery 200\n",
      "redscarepod 200\n",
      "photoshop 200\n",
      "options 200\n",
      "UnethicalLifeProTips 200\n",
      "crochet 200\n",
      "Sidemen 200\n",
      "whatcarshouldIbuy 200\n",
      "TombRaider 200\n",
      "aww 200\n",
      "okbuddyhololive 200\n",
      "indieheads 200\n",
      "motorcycle 200\n",
      "AusMemes 200\n",
      "askcarsales 200\n",
      "antiwork 200\n",
      "Eldenring 200\n",
      "PokemonInfiniteFusion 200\n",
      "Granblue_en 200\n",
      "bostonceltics 200\n",
      "BMW 200\n",
      "Infographics 200\n",
      "paradoxplaza 200\n",
      "ftm 200\n",
      "ZephyrusG14 200\n",
      "PrequelMemes 200\n",
      "TattooDesigns 200\n",
      "FoundryVTT 200\n",
      "Megaten 200\n",
      "unitedairlines 200\n",
      "snowboarding 200\n",
      "daddit 200\n",
      "Parenting 200\n",
      "agedlikemilk 200\n",
      "travel 200\n",
      "thinkpad 200\n",
      "PcBuild 200\n",
      "keto 200\n",
      "kindle 200\n",
      "Daytrading 200\n",
      "dating_advice 200\n",
      "drawing 200\n",
      "Ubuntu 200\n",
      "DIYUK 200\n",
      "ScammerPayback 200\n",
      "sciencememes 200\n",
      "javascript 200\n",
      "digitalnomad 200\n",
      "StardewValley 200\n",
      "russian 200\n",
      "technicalminecraft 200\n",
      "kpophelp 200\n",
      "BocchiTheRock 200\n",
      "reactjs 200\n",
      "Winnipeg 200\n",
      "deadcells 200\n",
      "financialindependence 200\n",
      "DC_Cinematic 200\n",
      "Drizzy 200\n",
      "razer 200\n",
      "AusPropertyChat 200\n",
      "USPS 200\n",
      "hockey 200\n",
      "MaliciousCompliance 200\n",
      "beyondthebump 200\n",
      "productivity 200\n",
      "Python 200\n",
      "WitchesVsPatriarchy 200\n",
      "ShitAmericansSay 200\n",
      "howto 200\n",
      "WorldOfWarships 200\n",
      "GTA 200\n",
      "Sacramento 200\n",
      "serbia 200\n",
      "dayz 200\n",
      "AskScienceFiction 200\n",
      "tolkienfans 200\n",
      "rolex 200\n",
      "chile 200\n",
      "teenagers 200\n",
      "taylorandtravis 200\n",
      "AllThatIsInteresting 200\n",
      "assettocorsa 200\n",
      "Yugioh101 200\n",
      "blenderhelp 200\n",
      "AzureLane 200\n",
      "philadelphia 200\n",
      "AMDHelp 200\n",
      "nintendo 200\n",
      "sto 200\n",
      "saltierthankrayt 200\n",
      "Philippines 200\n",
      "SBCGaming 200\n",
      "desabafos 200\n",
      "graphic_design 200\n",
      "FORTnITE 200\n",
      "learnart 200\n",
      "desimemes 200\n",
      "RedDeadOnline 200\n",
      "climbing 200\n",
      "MouseReview 200\n",
      "northernlion 200\n",
      "ExplainTheJoke 200\n",
      "TrueSTL 200\n",
      "europe 200\n",
      "PleX 200\n",
      "IAmTheMainCharacter 200\n",
      "solotravel 200\n",
      "BreakUps 200\n",
      "HunterXHunter 200\n",
      "UofT 200\n",
      "Mattress 200\n",
      "southpark 200\n",
      "blackmagicfuckery 200\n",
      "neovim 200\n",
      "overemployed 200\n",
      "AnalogCommunity 200\n",
      "CasualUK 200\n",
      "worldnews 200\n",
      "Satisfyingasfuck 200\n",
      "SGExams 200\n",
      "SolidWorks 200\n",
      "Filmmakers 200\n",
      "PetiteFashionAdvice 200\n",
      "batman 200\n",
      "fnv 200\n",
      "GODZILLA 200\n",
      "animepiracy 200\n",
      "EliteDangerous 200\n",
      "VitaPiracy 200\n",
      "careeradvice 200\n",
      "canucks 200\n",
      "3d6 200\n",
      "Economics 200\n",
      "ios 200\n",
      "CuratedTumblr 200\n",
      "AskFeminists 200\n",
      "KafkaMains 200\n",
      "PS3 200\n",
      "WWEGames 200\n",
      "CompetitiveWoW 200\n",
      "NewTubers 200\n",
      "RBI 200\n",
      "TwoBestFriendsPlay 200\n",
      "delta 200\n",
      "boxoffice 200\n",
      "needforspeed 200\n",
      "China 200\n",
      "mbti 200\n",
      "apple 200\n",
      "BuyItForLife 200\n",
      "houston 200\n",
      "canada 200\n",
      "hoi4 200\n",
      "books 200\n",
      "sydney 200\n",
      "confessions 200\n",
      "masskillers 200\n",
      "UpliftingNews 200\n",
      "androidapps 200\n",
      "LocalLLaMA 200\n",
      "IsItBullshit 200\n",
      "HolUp 200\n",
      "LenovoLegion 200\n",
      "GranblueFantasyRelink 200\n",
      "Android 200\n",
      "Canada_sub 200\n",
      "starsector 200\n",
      "puppy101 200\n",
      "newjersey 200\n",
      "Tokyo 200\n",
      "Homebuilding 200\n",
      "PokeInvesting 200\n",
      "hockeyplayers 200\n",
      "thefighterandthekid 200\n",
      "whatisthiscar 200\n",
      "CrackSupport 200\n",
      "PathOfExileBuilds 200\n",
      "Ultralight 200\n",
      "Dragonballsuper 200\n",
      "Maya 200\n",
      "Genshin_Memepact 200\n",
      "Warhammer40k 200\n",
      "worldbuilding 200\n",
      "MakeupAddiction 200\n",
      "aoe2 200\n",
      "greentext 200\n",
      "FreeGameFindings 200\n",
      "Denver 200\n",
      "chess 200\n",
      "germany 200\n",
      "askmath 200\n",
      "TheLastAirbender 200\n",
      "RaidShadowLegends 200\n",
      "HomeNetworking 200\n",
      "CompetitiveTFT 200\n",
      "Austin 200\n",
      "vegan 200\n",
      "PokemonTCG 200\n",
      "TaylorSwiftJets 200\n",
      "ShouldIbuythisgame 200\n",
      "SonicTheHedgehog 200\n",
      "HighStrangeness 200\n",
      "PoliticalHumor 200\n",
      "WeAreTheMusicMakers 200\n",
      "CoDCompetitive 200\n",
      "wallstreetbets 200\n",
      "investing 200\n",
      "gachagaming 200\n",
      "Boxing 200\n",
      "law 200\n",
      "AskReddit 204\n",
      "cocktails 200\n",
      "newzealand 200\n",
      "NikkeMobile 200\n",
      "ModernWarfareII 200\n",
      "fightsticks 200\n",
      "Starlink 200\n",
      "learnpython 200\n",
      "indiasocial 200\n",
      "OhNoConsequences 200\n",
      "yuzu 200\n",
      "regularcarreviews 200\n",
      "borderlands3 200\n",
      "Teachers 200\n",
      "AmItheEx 200\n",
      "Reverse1999 200\n",
      "lifehacks 200\n",
      "Professors 200\n",
      "codes 200\n",
      "Stormlight_Archive 200\n",
      "subaru 200\n",
      "CompetitiveApex 200\n",
      "NotHowGirlsWork 200\n",
      "Xennials 200\n",
      "formula1 200\n",
      "flightsim 200\n",
      "csharp 200\n",
      "leagueoflegends 200\n",
      "Bitcoin 200\n",
      "Genshin_Impact 200\n",
      "shittyfoodporn 200\n",
      "SCCM 200\n",
      "Monsterverse 200\n",
      "blunderyears 200\n",
      "Flights 200\n",
      "nhl 200\n",
      "Canning 200\n",
      "Fallout 200\n",
      "MacOS 200\n",
      "GameTheorists 200\n",
      "PokemonScarletViolet 200\n",
      "projectzomboid 200\n",
      "FirstTimeHomeBuyer 200\n",
      "britishcolumbia 200\n",
      "manga 200\n",
      "ContagiousLaughter 200\n",
      "lgbt 200\n",
      "PremierLeague 200\n",
      "TrueFilm 200\n",
      "UBC 200\n",
      "NatureIsFuckingLit 200\n",
      "nextjs 200\n",
      "furry 200\n",
      "OshiNoKo 200\n",
      "spicy 200\n",
      "mmamemes 200\n",
      "SubredditDrama 200\n",
      "MurderedByWords 200\n",
      "progresspics 200\n",
      "LateStageCapitalism 200\n",
      "FireEmblemHeroes 200\n",
      "antitrampo 200\n",
      "scambait 200\n",
      "Gunpla 200\n",
      "Enshrouded 200\n",
      "InstaCelebsGossip 200\n",
      "qBittorrent 200\n",
      "LifeProTips 200\n",
      "Coffee 200\n",
      "interviews 200\n",
      "terriblefacebookmemes 200\n",
      "KDRAMA 200\n",
      "interestingasfuck 200\n",
      "tmobile 200\n",
      "HermitCraft 200\n",
      "AskACanadian 200\n",
      "electricvehicles 200\n",
      "baldursgate 200\n",
      "HydroHomies 200\n",
      "nbacirclejerk 200\n",
      "mac 200\n",
      "OLED_Gaming 200\n",
      "MMORPG 200\n",
      "AskOldPeople 200\n",
      "GenP 200\n",
      "HeadphoneAdvice 200\n",
      "firefox 200\n",
      "AskHistory 200\n",
      "WarhammerFantasy 200\n",
      "arbeitsleben 200\n",
      "gamecollecting 200\n",
      "kingdomcome 200\n",
      "Garmin 200\n",
      "camphalfblood 200\n",
      "HarryPotterGame 200\n",
      "phinvest 200\n",
      "Celebs 200\n",
      "adultingph 200\n",
      "streetwear 200\n",
      "fortinet 200\n",
      "Ultrakill 200\n",
      "WaltDisneyWorld 200\n",
      "preppers 200\n",
      "comicbookmovies 200\n",
      "Audi 200\n",
      "MapPorn 200\n",
      "stopdrinking 200\n",
      "rupaulsdragrace 200\n",
      "EnoughMuskSpam 200\n",
      "microsoft 200\n",
      "Howtolooksmax 200\n",
      "BORUpdates 200\n",
      "Silksong 200\n",
      "Sephora 200\n",
      "dankmemes 200\n",
      "admincraft 200\n",
      "nuzlocke 200\n",
      "Patriots 200\n",
      "TheBoys 200\n",
      "RoastMe 200\n",
      "brasil 200\n",
      "flightradar24 200\n",
      "LegendsOfRuneterra 200\n",
      "billsimmons 200\n",
      "indianbikes 200\n",
      "handbags 200\n",
      "godot 200\n",
      "Killtony 200\n",
      "lawschooladmissions 200\n",
      "EngineeringStudents 200\n",
      "Screenwriting 200\n",
      "TrashTaste 200\n",
      "Persona5 200\n",
      "summonerschool 200\n",
      "GetMotivated 200\n",
      "OnePiecePowerScaling 200\n",
      "SuccessionTV 200\n",
      "malelivingspace 200\n",
      "destiny2 200\n",
      "fountainpens 200\n",
      "smallbusiness 200\n",
      "HadesTheGame 200\n",
      "DeadBedrooms 200\n",
      "CultOfTheLamb 200\n",
      "AskHistorians 200\n",
      "AskWomenOver30 200\n",
      "skyrimmods 200\n",
      "CODWarzone 200\n",
      "virtualreality 200\n",
      "SWGalaxyOfHeroes 200\n",
      "WTF 200\n",
      "math 200\n",
      "aiArt 200\n",
      "Twitter 200\n",
      "LawSchool 200\n",
      "OlderThanYouThinkIAm 200\n",
      "coolguides 200\n",
      "Brooklyn 200\n",
      "curb 200\n",
      "Portland 200\n",
      "criterion 200\n",
      "Stargate 200\n",
      "ElectricalEngineering 200\n",
      "melbourne 200\n",
      "CollegeRant 200\n",
      "TheSimpsons 200\n",
      "MagicArena 200\n",
      "asktransgender 200\n",
      "legaladvicecanada 200\n",
      "Columbus 200\n",
      "CODZombies 200\n",
      "betterCallSaul 200\n",
      "windows 200\n",
      "skyrim 200\n",
      "devsarg 200\n",
      "airpods 200\n",
      "howyoudoin 200\n",
      "samsung 200\n",
      "IRS 200\n",
      "iamatotalpieceofshit 200\n",
      "rickandmorty 200\n",
      "Kengan_Ashura 200\n",
      "ffxiv 200\n",
      "Seattle 200\n",
      "reddit.com 200\n",
      "youtubers 200\n",
      "debtfree 200\n",
      "bodybuilding 200\n",
      "outerwilds 200\n",
      "BollyBlindsNGossip 200\n",
      "Cartalk 200\n",
      "ontario 200\n",
      "bigdickproblems 200\n",
      "imaginarymaps 200\n",
      "castiron 200\n",
      "fidelityinvestments 200\n",
      "Dimension20 200\n",
      "announcements 200\n",
      "Planetside 200\n",
      "linkedin 200\n",
      "titanfall 200\n",
      "dataengineering 200\n",
      "silenthill 200\n",
      "facepalm 200\n",
      "VietNam 200\n",
      "sewing 200\n",
      "nursing 200\n",
      "WorldofTanks 200\n",
      "laptops 200\n",
      "atheism 200\n",
      "AskFrance 200\n",
      "netflix 200\n",
      "shittytattoos 200\n",
      "ModernWarfareIII 200\n",
      "arduino 200\n",
      "self 200\n",
      "CharacterRant 200\n",
      "ShitpostXIV 200\n",
      "OnePiece 200\n",
      "mildlyinteresting 200\n",
      "macapps 200\n",
      "blackcats 200\n",
      "USCIS 200\n",
      "datingoverthirty 200\n",
      "The10thDentist 200\n",
      "FiftyFifty 200\n",
      "Chainsawfolk 200\n",
      "Argaming 200\n",
      "MechanicalKeyboards 200\n",
      "Palworld 200\n",
      "iRacing 200\n",
      "overclocking 200\n",
      "operabrowser 200\n",
      "fromsoftware 200\n",
      "cscareerquestions 200\n",
      "neoliberal 200\n",
      "mountandblade 200\n",
      "leetcode 200\n",
      "OnePieceTCG 200\n",
      "puzzles 200\n",
      "entertainment 200\n",
      "relationship_advice 200\n",
      "fireemblem 200\n",
      "HaloStory 200\n",
      "Warzone 200\n",
      "Biohackers 200\n",
      "intel 200\n",
      "suggestmeabook 200\n",
      "LoveIsBlindOnNetflix 200\n",
      "Calgary 200\n",
      "SatisfactoryGame 200\n",
      "army 200\n",
      "CalamityMod 200\n",
      "ireland 200\n",
      "homeowners 200\n",
      "ClipStudio 200\n",
      "Seaofthieves 200\n",
      "Letterboxd 200\n",
      "Piracy 200\n",
      "Doom 200\n",
      "Meditation 200\n",
      "Maplestory 200\n",
      "cs2 200\n",
      "Animesuggest 200\n",
      "AZURE 200\n",
      "Frieren 200\n",
      "devops 200\n",
      "AskAGerman 200\n",
      "KGBTR 200\n",
      "RealTesla 200\n",
      "ShingekiNoKyojin 200\n",
      "TalesFromTheFrontDesk 200\n",
      "shittydarksouls 200\n",
      "careerguidance 200\n",
      "Crunchyroll 200\n",
      "Target 200\n",
      "wizardposting 200\n",
      "1200isplenty 200\n",
      "barstoolsports 200\n",
      "FantasyPL 200\n",
      "LiesOfP 200\n",
      "kansascity 200\n",
      "NonPoliticalTwitter 200\n",
      "FortNiteBR 200\n",
      "JEENEETards 200\n",
      "overlord 200\n",
      "DragonsDogma 200\n",
      "Invincible 200\n",
      "Judaism 200\n",
      "stupidquestions 200\n",
      "Seahawks 200\n",
      "pettyrevenge 200\n",
      "kubernetes 200\n",
      "AutismInWomen 200\n",
      "chromeos 200\n",
      "jerma985 200\n",
      "PiratedGames 200\n",
      "China_irl 200\n",
      "SwiftlyNeutral 200\n",
      "superman 200\n",
      "steak 200\n",
      "Ubiquiti 200\n",
      "recruitinghell 200\n",
      "guitarpedals 200\n",
      "Minecraft 200\n",
      "gamedev 200\n",
      "dating 200\n",
      "Concrete 200\n",
      "Piratefolk 200\n",
      "CreditCards 200\n",
      "RimWorld 200\n",
      "EscapefromTarkov 200\n",
      "TwoSentenceHorror 200\n",
      "madisonwi 200\n",
      "askphilosophy 200\n",
      "ottawa 200\n",
      "PHCreditCards 200\n",
      "audioengineering 200\n",
      "interesting 200\n",
      "adhdwomen 200\n",
      "Broadway 200\n",
      "ChoosingBeggars 200\n",
      "StardustCrusaders 200\n",
      "BeamNG 200\n",
      "ImTheMainCharacter 200\n",
      "synology 200\n",
      "attackontitan 200\n",
      "WWE 200\n",
      "AskMiddleEast 200\n",
      "Finanzen 200\n",
      "AmITheDevil 200\n",
      "ImmigrationCanada 200\n",
      "BlackPeopleTwitter 200\n",
      "childfree 200\n",
      "darksouls 200\n",
      "GetNoted 200\n",
      "Games 200\n",
      "csgo 200\n",
      "java 200\n",
      "thalassophobia 200\n",
      "truegaming 200\n",
      "Standup 200\n",
      "DivinityOriginalSin 200\n",
      "WRX 200\n",
      "PublicFreakout 200\n",
      "goodanimemes 200\n",
      "smoking 200\n",
      "MechanicAdvice 200\n",
      "islam 200\n",
      "blender 200\n",
      "DBZDokkanBattle 200\n",
      "australian 200\n",
      "Celebhub 200\n",
      "dankruto 200\n",
      "Roms 200\n",
      "UrbanHell 200\n",
      "montreal 200\n",
      "danganronpa 200\n",
      "HalfLife 200\n",
      "startups 200\n",
      "AskLosAngeles 200\n",
      "AreTheStraightsOK 200\n",
      "youtube 200\n",
      "MovieDetails 200\n",
      "survivor 200\n",
      "WorkReform 200\n",
      "thesopranos 200\n",
      "RedLetterMedia 200\n",
      "tearsofthekingdom 200\n",
      "Ben10 200\n",
      "BlueLock 200\n",
      "starcraft 200\n",
      "thefinals 200\n",
      "unRAID 200\n",
      "UniUK 200\n",
      "lego 200\n",
      "darkestdungeon 200\n",
      "RetroArch 200\n",
      "onlyfansadvice 200\n",
      "musictheory 200\n",
      "korea 200\n",
      "Firearms 200\n",
      "electricians 200\n",
      "3Dprinting 200\n",
      "roblox 200\n",
      "AskMeuf 200\n",
      "socialskills 200\n",
      "halo 200\n",
      "Madden 200\n",
      "collapse 200\n",
      "bangalore 200\n",
      "LegalAdviceUK 200\n",
      "IndianGaming 200\n",
      "FanTheories 200\n",
      "piano 200\n",
      "creepy 200\n",
      "tax 200\n",
      "TheSilphRoad 200\n",
      "darksouls3 200\n",
      "Overwatch 200\n",
      "redditmoment 200\n",
      "managers 200\n",
      "AskEngineers 200\n",
      "programming 200\n",
      "help 200\n",
      "Physics 200\n",
      "videos 200\n",
      "browsers 200\n",
      "2007scape 200\n",
      "sandiego 200\n",
      "sportsbook 200\n",
      "Pandabuy 200\n",
      "Eyebleach 200\n",
      "gifs 200\n",
      "hypotheticalsituation 200\n",
      "OneOrangeBraincell 200\n",
      "SaintMeghanMarkle 200\n",
      "ETFs 200\n",
      "thesims 200\n",
      "FireflyMains 200\n",
      "ITCareerQuestions 200\n",
      "AskSF 200\n",
      "Pathfinder_Kingmaker 200\n",
      "DnDHomebrew 200\n",
      "CivVI 200\n",
      "SkullAndBonesGame 200\n",
      "yugioh 200\n",
      "homeautomation 200\n",
      "sunraybee 200\n",
      "tacticalgear 200\n",
      "FashionReps 200\n",
      "Hiphopcirclejerk 200\n",
      "ChikaPH 200\n",
      "mtgfinance 200\n",
      "Corsair 200\n",
      "Vinesauce 200\n",
      "juridischadvies 200\n",
      "AmITheAngel 200\n",
      "playboicarti 200\n",
      "clevercomebacks 200\n",
      "AITAH 200\n",
      "HollowKnight 200\n",
      "JeffArcuri 200\n",
      "reactnative 200\n",
      "HonkaiStarRail_leaks 200\n",
      "runescape 200\n",
      "excel 200\n",
      "PS4 200\n",
      "ukpolitics 200\n",
      "Prison 200\n",
      "splatoon 200\n",
      "UPS 200\n",
      "ARAM 200\n",
      "breakingbad 200\n",
      "bloodborne 200\n",
      "NoMansSkyTheGame 200\n",
      "xmen 200\n",
      "conspiracy 200\n",
      "WerWieWas 200\n",
      "Starfield 200\n",
      "MtF 200\n",
      "debian 200\n",
      "gigabyte 200\n",
      "intermittentfasting 200\n",
      "audiophile 200\n",
      "sanfrancisco 200\n",
      "ThatsInsane 200\n",
      "NoStupidQuestions 201\n",
      "StLouis 200\n",
      "Supplements 200\n",
      "GatekeepingYuri 200\n",
      "alberta 200\n",
      "tall 200\n",
      "Morrowind 200\n",
      "RPClipsGTA 200\n",
      "AskMenOver30 200\n",
      "news 200\n",
      "royalcaribbean 200\n",
      "SPTarkov 200\n",
      "ProgrammerHumor 200\n",
      "loseit 200\n",
      "Smite 200\n",
      "PropagandaPosters 200\n",
      "CleaningTips 200\n",
      "PSVR 200\n",
      "OverwatchUniversity 200\n",
      "CarsAustralia 200\n",
      "memesopdidnotlike 200\n",
      "awardtravel 200\n",
      "fantasyfootball 200\n",
      "animememes 200\n",
      "duolingo 200\n",
      "edmproduction 200\n",
      "bayarea 200\n",
      "LearnJapanese 200\n",
      "Naruto 200\n",
      "gradadmissions 200\n",
      "Helldivers 200\n",
      "6thForm 200\n",
      "phoenix 200\n",
      "vosfinances 200\n",
      "sidehustle 200\n",
      "TeamfightTactics 200\n",
      "csgomarketforum 200\n",
      "cordcutters 200\n",
      "SEO 200\n",
      "consulting 200\n",
      "MonsterHunterMeta 200\n",
      "BaldursGate3 201\n",
      "Israel 200\n",
      "DCcomics 200\n",
      "h3h3productions 200\n",
      "HobbyDrama 200\n",
      "Gundam 200\n",
      "blackdesertonline 200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = get_combined_embeddings([])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# df.to_pickle(\"embeddings.pickle\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Limit to first 1000 embeddings\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# len(df)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msub_df\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfor_each_sub_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msub_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSub\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msub_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 51\u001b[0m, in \u001b[0;36mfor_each_sub_embeddings\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m file_list \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_list:\n\u001b[0;32m---> 51\u001b[0m     df_temp \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m df_temp\n",
      "File \u001b[0;32m~/src/findthesub/.venv/lib/python3.13/site-packages/pandas/io/pickle.py:202\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;66;03m# We want to silence any warnings about, e.g. moved modules.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mWarning\u001b[39;00m)\n\u001b[0;32m--> 202\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32minternals.pyx:629\u001b[0m, in \u001b[0;36mpandas._libs.internals._unpickle_block\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/src/findthesub/.venv/lib/python3.13/site-packages/pandas/core/internals/blocks.py:2645\u001b[0m, in \u001b[0;36mmaybe_coerce_values\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   2638\u001b[0m     \u001b[38;5;18m__slots__\u001b[39m \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   2641\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m   2642\u001b[0m \u001b[38;5;66;03m# Constructor Helpers\u001b[39;00m\n\u001b[0;32m-> 2645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmaybe_coerce_values\u001b[39m(values: ArrayLike) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[1;32m   2646\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2647\u001b[0m \u001b[38;5;124;03m    Input validation for values passed to __init__. Ensure that\u001b[39;00m\n\u001b[1;32m   2648\u001b[0m \u001b[38;5;124;03m    any datetime64/timedelta64 dtypes are in nanoseconds.  Ensure\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2657\u001b[0m \u001b[38;5;124;03m    values : np.ndarray or ExtensionArray\u001b[39;00m\n\u001b[1;32m   2658\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2659\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring NumpyExtensionArray is already extracted.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# df = get_combined_embeddings([])\n",
    "# df.to_pickle(\"embeddings.pickle\")\n",
    "\n",
    "# Print numeber of embeddings\n",
    "# print(f\"Total embeddings: {len(df)}\")\n",
    "\n",
    "# Limit to first 1000 embeddings\n",
    "# len(df)\n",
    "\n",
    "for sub_df in for_each_sub_embeddings():\n",
    "    print(sub_df[\"Sub\"].iloc[0], len(sub_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc97db60df841bba3151810fe02f92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='0/0 processed')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2aa034646354fc9a8d28c91397835e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Chunks')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4a1babb34c4113b911a9612d4a3438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Logs', disabled=True, layout=Layout(height='300px', width='100%'), placeholderâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, math, json, requests\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create and display widgets\n",
    "progress = widgets.IntProgress(value=0, description='Chunks')\n",
    "progress_label = widgets.Label(value=\"0/0 processed\")\n",
    "log_area = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"Logs will appear here...\",\n",
    "    description=\"Logs\",\n",
    "    layout=widgets.Layout(width='100%', height='300px'),\n",
    "    disabled=True\n",
    ")\n",
    "display(progress_label, progress, log_area)\n",
    "\n",
    "# Configuration Parameters\n",
    "API_URL = f\"{os.environ.get('ZILLIZ_HOST')}/v2/vectordb/entities/insert\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {os.environ.get('ZILLIZ_API_KEY')}\",\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "COLLECTION_NAME = \"FindTheSub\"\n",
    "CHUNK_SIZE = 250\n",
    "\n",
    "def send_data_to_api(df, url, headers, collection_name, chunk_size=100,\n",
    "                     progress=None, progress_label=None, log_area=None):\n",
    "    \"\"\"\n",
    "    Sends data from a DataFrame to the specified API in chunks.\n",
    "    - df (pd.DataFrame): DataFrame containing 'Sub', 'Title', and 'Embedding' columns.\n",
    "    - url (str): The API endpoint URL.\n",
    "    - headers (dict): HTTP headers for the request.\n",
    "    - collection_name (str): Name of the collection in the API.\n",
    "    - chunk_size (int): Number of records to send per API request.\n",
    "    - progress, progress_label, log_area: optional widgets for progress/UI logging\n",
    "    \"\"\"\n",
    "    \n",
    "    total_records = len(df)\n",
    "    total_chunks = math.ceil(total_records / chunk_size)\n",
    "\n",
    "    if log_area:\n",
    "        log_area.value += f\"Total records to send: {total_records}\\n\"\n",
    "        log_area.value += f\"Sending in {total_chunks} chunk(s) of up to {chunk_size} records each.\\n\\n\"\n",
    "    else:\n",
    "        print(f\"Total records to send: {total_records}\")\n",
    "        print(f\"Sending in {total_chunks} chunk(s) of up to {chunk_size} records each.\\n\")\n",
    "    \n",
    "    for chunk_num in range(total_chunks):\n",
    "        start_idx = chunk_num * chunk_size\n",
    "        end_idx = min(start_idx + chunk_size, total_records)\n",
    "        chunk_df = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        data_payload = []\n",
    "        for idx, row in chunk_df.iterrows():\n",
    "            entity = {\n",
    "                \"primary_key\": hash(f\"{row['Sub']}_{row['Title']}\"),\n",
    "                \"post_id\": row['Id'],\n",
    "                \"sub\": row['Sub'],\n",
    "                \"title\": row['Title'],\n",
    "                \"vector\": row['Embedding']\n",
    "            }\n",
    "            data_payload.append(entity)\n",
    "        \n",
    "        payload = {\n",
    "            \"collectionName\": collection_name,\n",
    "            \"data\": data_payload\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "            \n",
    "            msg = (\n",
    "                f\"Chunk {chunk_num + 1}/{total_chunks} (Records {start_idx} to {end_idx - 1}) sent successfully.\\n\"\n",
    "                f\"Response: {json.dumps(response_data, indent=2)[0:64]}\\n\\n\"\n",
    "            )\n",
    "            if log_area:\n",
    "                log_area.value += msg\n",
    "            else:\n",
    "                print(msg)\n",
    "                \n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            msg = (f\"HTTP error occurred for chunk {chunk_num + 1}: {http_err}\\n\"\n",
    "                   f\"Response: {response.text}\\n\\n\")\n",
    "            if log_area:\n",
    "                log_area.value += msg\n",
    "            else:\n",
    "                print(msg)\n",
    "        except Exception as err:\n",
    "            msg = f\"An error occurred for chunk {chunk_num + 1}: {err}\\n\\n\"\n",
    "            if log_area:\n",
    "                log_area.value += msg\n",
    "            else:\n",
    "                print(msg)\n",
    "\n",
    "\n",
    "# Example Usage (assuming df is your DataFrame):\n",
    "progress.value = 0\n",
    "progress.max = len(subs_to_fetch)\n",
    "progress_label.value = f\"0/{len(subs_to_fetch)} processed\"\n",
    "for sub_df in for_each_sub_embeddings():\n",
    "    log_area.value += f\"Sending data for '{sub_df['Sub'].iloc[0]}' to the API...\\n\"\n",
    "    send_data_to_api(sub_df, API_URL, HEADERS, COLLECTION_NAME, CHUNK_SIZE, log_area=log_area)\n",
    "    progress.value += 1\n",
    "    progress_label.value = f\"{progress.value}/{progress.max} processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sub                                                 artificial\n",
       "Title                                        AI has hit a wall\n",
       "Embedding    [-0.01548602432012558, -0.010631220415234566, ...\n",
       "Added                               2024-12-24T17:35:03.596191\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_pickle('embeddings.pickle')\n",
    "\n",
    "# Replace \"Embedding\" with \"Embedding_np\"\n",
    "# df[\"Embedding_np\"] = df[\"Embedding\"].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "# df = df.drop(columns=[\"Embedding\"])  # Drop the original \"Embedding\" column\n",
    "# df.rename(columns={\"Embedding_np\": \"Embedding\"}, inplace=True)  # Rename \"Embedding_np\" to \"Embedding\"\n",
    "\n",
    "# # Write back to a Parquet file\n",
    "# df.to_pickle('server/embeddings.pickle')\n",
    "\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_closest_subs(df, txt, n=10):\n",
    "    embedding = np.array(get_embedding([txt])[0].embedding)\n",
    "    df['similarities'] = df.Embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "    # Aggregate similarities by sub, then return top n subs\n",
    "    grouped = df.groupby('Sub')['similarities'].mean().reset_index()\n",
    "    return grouped.sort_values('similarities', ascending=False).head(n)\n",
    "\n",
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>askSingapore</td>\n",
       "      <td>0.163058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mtg</td>\n",
       "      <td>0.148387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adulting</td>\n",
       "      <td>0.147420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nyc</td>\n",
       "      <td>0.143336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LeopardsAteMyFace</td>\n",
       "      <td>0.142655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sub  distance\n",
       "12       askSingapore  0.163058\n",
       "17                mtg  0.148387\n",
       "1            Adulting  0.147420\n",
       "19                nyc  0.143336\n",
       "3   LeopardsAteMyFace  0.142655"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notebook Cell\n",
    "\n",
    "SEARCH_API_URL = f\"{os.environ.get(\"ZILLIZ_HOST\")}/v2/vectordb/entities/search\"\n",
    "SEARCH_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {os.environ.get('ZILLIZ_API_KEY')}\",\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "COLLECTION_NAME = \"FindTheSub\"\n",
    "LIMIT = 10\n",
    "TOP_N = 5\n",
    "\n",
    "def query_closest_subs(txt, limit=LIMIT, top_n=TOP_N):\n",
    "    embeddings = get_embedding([txt])\n",
    "    if not embeddings: raise ValueError(\"Failed to obtain embedding for the input text.\")\n",
    "    payload = {\n",
    "        \"collectionName\": COLLECTION_NAME,\n",
    "        \"data\": [embeddings[0].embedding],\n",
    "        \"limit\": limit,\n",
    "        \"outputFields\": [\"sub\"]\n",
    "    }\n",
    "    r = requests.post(SEARCH_API_URL, json.dumps(payload), headers=SEARCH_HEADERS)\n",
    "    r.raise_for_status()\n",
    "    df = pd.DataFrame(r.json().get('data', []))\n",
    "    if df.empty: \n",
    "        return df\n",
    "    grouped = df.groupby('sub')['distance'].mean().reset_index()\n",
    "    return grouped.sort_values('distance', ascending=False).head(top_n)\n",
    "\n",
    "result = query_closest_subs(\"Why did I get laid off before the holidays?\", limit=1000)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
