{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "import praw\n",
    "import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"), \n",
    ")\n",
    "\n",
    "def get_embedding(txts):\n",
    "    response = client.embeddings.create(\n",
    "        input=txts,\n",
    "        model=\"text-embedding-3-small\"\n",
    "        )\n",
    "    return response.data\n",
    "\n",
    "# response = client.embeddings.create(\n",
    "#     input=\"Your text string goes here\",\n",
    "#     model=\"text-embedding-3-small\"\n",
    "# # )\n",
    "\n",
    "# print(response.data[0].embedding)\n",
    "embeddings_dir = \"embeddings/\"\n",
    "\n",
    "reddit_subs_limit = 1200\n",
    "reddit_top_all_post_limit = 20\n",
    "reddit_hot_post_limit = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings_for_titles(sub, titles, chunk_size=25):\n",
    "    sub_csv_filename = f\"{embeddings_dir}{sub}.pickle\"\n",
    "    if not os.path.exists(sub_csv_filename):\n",
    "        existing_df = pd.DataFrame(columns=[\"Sub\", \"Title\", \"Embedding\", \"Added\"])\n",
    "    else:\n",
    "        existing_df = pd.read_pickle(sub_csv_filename)\n",
    "\n",
    "    existing = set(zip(existing_df[\"Sub\"], existing_df[\"Title\"]))\n",
    "    new_titles = [t for t in titles if (sub, t) not in existing]\n",
    "    for i in range(0, len(new_titles), chunk_size):\n",
    "        chunk = new_titles[i:i+chunk_size]\n",
    "        results = get_embedding(chunk)\n",
    "        for title, res in zip(chunk, results):\n",
    "            existing_df.loc[len(existing_df)] = [\n",
    "                sub,\n",
    "                title,\n",
    "                res.embedding,\n",
    "                datetime.datetime.now().isoformat()\n",
    "            ]\n",
    "\n",
    "    existing_df.to_pickle(sub_csv_filename)\n",
    "    return {\n",
    "        \"fetched\": len(new_titles),\n",
    "        \"skipped\": len(titles) - len(new_titles),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def get_combined_embeddings(only_subs):\n",
    "    file_list = glob.glob(f\"{embeddings_dir}*.pickle\")\n",
    "    all_dfs = []\n",
    "    for file_path in file_list:\n",
    "        if len(only_subs) > 0:\n",
    "            should_read = False\n",
    "            for sub in only_subs:\n",
    "                if file_path.endswith(f\"{sub}.pickle\"):\n",
    "                    should_read = True\n",
    "                    break\n",
    "            if not should_read:\n",
    "                continue\n",
    "        df_temp = pd.read_pickle(file_path)\n",
    "        all_dfs.append(df_temp)\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    # combined_df[\"Embedding_np\"] = combined_df[\"Embedding\"].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=os.environ.get(\"REDDIT_CLIENT_ID\"),\n",
    "    client_secret=os.environ.get(\"REDDIT_CLIENT_SECRET\"),\n",
    "    user_agent='Python:com.findthesub:fetch-script'\n",
    ")\n",
    "\n",
    "def fetch_hot_and_top_posts(sub, config = {\n",
    "    'hot_count': reddit_hot_post_limit,\n",
    "    'top_all_count': reddit_top_all_post_limit\n",
    "}):\n",
    "    max_chunk = 100\n",
    "    hot_count = config['hot_count']\n",
    "    top_all_count = config['top_all_count']\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    collected = []\n",
    "    for sort, limit, kwargs in [\n",
    "        ('hot', hot_count, {}),\n",
    "        ('top', top_all_count, {'time_filter':'all'})\n",
    "    ]:\n",
    "        after = None\n",
    "        fetched = 0\n",
    "        while fetched < limit:\n",
    "            chunk_size = min(max_chunk, limit - fetched)\n",
    "            gen = getattr(subreddit, sort)(limit=chunk_size, params={'after': after}, **kwargs)\n",
    "            items = list(gen)\n",
    "            if not items: break\n",
    "            collected.extend(items)\n",
    "            fetched += len(items)\n",
    "            after = items[-1].name\n",
    "            if len(items) < chunk_size: break\n",
    "    return [p.title for p in collected]\n",
    "\n",
    "    return [p.title for p in collected]\n",
    "\n",
    "def get_top_subs(limit=reddit_subs_limit):\n",
    "    max_chunk = 100\n",
    "    collected = []\n",
    "    after = None\n",
    "    \n",
    "    fetched = 0\n",
    "    while fetched < limit:\n",
    "        chunk_size = min(max_chunk, limit - fetched)\n",
    "        gen = reddit.subreddits.popular(limit=chunk_size, params={'after': after})\n",
    "        items = list(gen)\n",
    "        if not items:\n",
    "            break\n",
    "        collected.extend(items)\n",
    "        fetched += len(items)\n",
    "        # For subreddits, the 'fullname' should be used as 'after'\n",
    "        # but if that doesn't work, try items[-1].name or items[-1].id\n",
    "        after = items[-1].fullname  \n",
    "        if len(items) < chunk_size:\n",
    "            break\n",
    "\n",
    "    return [s.display_name for s in collected]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# titles = fetch_reddit_post_titles_for_sub('python', limit=120, sort='new')\n",
    "# print(titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ebeb2e7fe54f1c8f3c78886a64e1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='0/0 processed')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c4880fe0544b1384fb07052971d993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e5085f407f45fba2a76e7fafe8f26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Logs', disabled=True, layout=Layout(height='300px', width='100%'), placeholderâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "progress = widgets.IntProgress(value=0, )\n",
    "progress_label = widgets.Label(value=f\"0/0 processed\")\n",
    "log_area = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"Logs will appear here...\",\n",
    "    description=\"Logs\",\n",
    "    layout=widgets.Layout(width='100%', height='300px'),\n",
    "    disabled=True,\n",
    ")\n",
    "display(progress_label, progress, log_area)\n",
    "\n",
    "log_area.value += \"Fetching top subreddits...\\n\"\n",
    "subs_to_fetch = get_top_subs()\n",
    "log_area.value += f\"Fetched {len(subs_to_fetch)} subreddits\\n\\n\"\n",
    "progress.max = len(subs_to_fetch)\n",
    "progress.value = 0\n",
    "progress_label.value = f\"0/{len(subs_to_fetch)} processed\"\n",
    "\n",
    "\n",
    "for sub in subs_to_fetch:\n",
    "    progress.value += 1 \n",
    "    progress_label.value = f\"{progress.value}/{len(subs_to_fetch)} processed\"\n",
    "    \n",
    "    # Update logs\n",
    "    log_message = f\"Processing '{sub}'...\\n\"\n",
    "    titles = fetch_hot_and_top_posts(sub)\n",
    "    log_message += f\"Fetched {len(titles)} titles for {sub}\\n\"\n",
    "    \n",
    "    results = add_embeddings_for_titles(sub, titles)\n",
    "    log_message += f\"Added {results['fetched']} new embeddings, skipped {results['skipped']} existing\\n\\n\"\n",
    "    \n",
    "    log_area.value += log_message  # Append to the textarea\n",
    "    \n",
    "    log_area_lines = log_area.value.split(\"\\n\")\n",
    "    if len(log_area_lines) > 500:\n",
    "        log_area.value = \"\\n\".join(log_area_lines[-500:])\n",
    "\n",
    "# Final step\n",
    "df = get_combined_embeddings(subs_to_fetch)\n",
    "df.to_pickle(\"embeddings.pickle\")\n",
    "log_area.value += \"Processing complete. Embeddings saved to 'embeddings.pickle'.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sub                                                 grandorder\n",
       "Title           [Help and Question Thread] - December 22, 2024\n",
       "Added                               2024-12-22T15:08:53.756286\n",
       "Embedding    [0.028386393561959267, 0.004223015159368515, -...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_pickle('server/embeddings.pickle')\n",
    "\n",
    "# Replace \"Embedding\" with \"Embedding_np\"\n",
    "# df[\"Embedding_np\"] = df[\"Embedding\"].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "# df = df.drop(columns=[\"Embedding\"])  # Drop the original \"Embedding\" column\n",
    "# df.rename(columns={\"Embedding_np\": \"Embedding\"}, inplace=True)  # Rename \"Embedding_np\" to \"Embedding\"\n",
    "\n",
    "# # Write back to a Parquet file\n",
    "# df.to_pickle('server/embeddings.pickle')\n",
    "\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_closest_subs(df, txt, n=10):\n",
    "    embedding = np.array(get_embedding([txt])[0].embedding)\n",
    "    df['similarities'] = df.Embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "    # Aggregate similarities by sub, then return top n subs\n",
    "    grouped = df.groupby('Sub')['similarities'].mean().reset_index()\n",
    "    return grouped.sort_values('similarities', ascending=False).head(n)\n",
    "\n",
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sub</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>GamingLeaksAndRumours</td>\n",
       "      <td>0.303896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>CrackWatch</td>\n",
       "      <td>0.280385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>anime_irl</td>\n",
       "      <td>0.271308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>Unity3D</td>\n",
       "      <td>0.259348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Games</td>\n",
       "      <td>0.253797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>FiftyFifty</td>\n",
       "      <td>0.253734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>PS5</td>\n",
       "      <td>0.252305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>XboxSeriesX</td>\n",
       "      <td>0.246258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>virtualreality</td>\n",
       "      <td>0.244820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>HonkaiStarRail_leaks</td>\n",
       "      <td>0.243416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Sub  similarities\n",
       "190  GamingLeaksAndRumours      0.303896\n",
       "116             CrackWatch      0.280385\n",
       "498              anime_irl      0.271308\n",
       "460                Unity3D      0.259348\n",
       "188                  Games      0.253797\n",
       "169             FiftyFifty      0.253734\n",
       "337                    PS5      0.252305\n",
       "485            XboxSeriesX      0.246258\n",
       "972         virtualreality      0.244820\n",
       "224   HonkaiStarRail_leaks      0.243416"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs = get_closest_subs(df, 'Latest leaks show crazy realism')\n",
    "subs\n",
    "# subs.to_json(orient='records')\n",
    "# subs.to_dict(orient='records')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
